{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 인공신경망 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.5586]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.9019], requires_grad=True)\n",
      "\n",
      "tensor([-0.3434], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.])\n",
    "model = nn.Linear(1,1) # 입력 node 한 개, 출력 node 한 개인 layer 만든다\n",
    "\n",
    "print(model.weight) # 만들면서 initilaize 해준다\n",
    "print(model.bias)\n",
    "print()\n",
    "\n",
    "y = model(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제 수식으로 비교해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3434], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x @ model.weight + model.bias\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "동일한 값인 것을 볼 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- fc1 -----\n",
      "Parameter containing:\n",
      "tensor([[-0.3988],\n",
      "        [ 0.1842],\n",
      "        [-0.1550]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1147,  0.6470,  0.3014], requires_grad=True)\n",
      "----- fc2 -----\n",
      "Parameter containing:\n",
      "tensor([[-0.1379,  0.2214,  0.0785]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.3519], requires_grad=True)\n",
      "\n",
      "tensor([-0.5135,  0.8312,  0.1464], grad_fn=<AddBackward0>)\n",
      "tensor([0.6183], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.])\n",
    "\n",
    "fc1 = nn.Linear(1,3) # input : 1, output : 3인 node로 구성 = fully-connected\n",
    "fc2 = nn.Linear(3,1)\n",
    "\n",
    "print(\"----- fc1 -----\")\n",
    "print(fc1.weight)\n",
    "print(fc1.bias)\n",
    "print(\"----- fc2 -----\")\n",
    "print(fc2.weight)\n",
    "print(fc2.bias)\n",
    "print()\n",
    "\n",
    "x1 = fc1(x)\n",
    "x2 = fc2(x1)\n",
    "print(x1)\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마찬가지로 수식으로!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x1 and 3x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/lee/Desktop/workspace/git/TIL/PYTORCH/Ch.2/MLP.ipynb 셀 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lee/Desktop/workspace/git/TIL/PYTORCH/Ch.2/MLP.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m y \u001b[39m=\u001b[39m (x\u001b[39m@fc1\u001b[39;49m\u001b[39m.\u001b[39;49mweight \u001b[39m+\u001b[39m fc1\u001b[39m.\u001b[39mbias)\u001b[39m@fc2\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m+\u001b[39m fc2\u001b[39m.\u001b[39mbias\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lee/Desktop/workspace/git/TIL/PYTORCH/Ch.2/MLP.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m## (1x1 @ 1x3 + 1x3)@ 3x1 + 1x1) ?\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x1 and 3x1)"
     ]
    }
   ],
   "source": [
    "y = (x@fc1.weight + fc1.bias)@fc2.weight + fc2.bias\n",
    "## (1x1 @ 1x3 + 1x3)@ 3x1 + 1x1) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "에러가 난다 !\n",
    "\n",
    "<img src=\"image/nn_linear.png\" width=\"600\">\n",
    "\n",
    "nn.Linear 안에서 Transpose가 되어있기 때문!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "왜 T 되어있느냐?\n",
    "- shape는 개x채x행x열 로 이루어져있다\n",
    "  - x = torch.tensor([1., 2., 3.]) => 1x3 = 1행 3열 = 1개 3채널 로도 볼 수 있다\n",
    "- nn.linear은 개x채x행x열 에서 input으로 \"채널\"이 들어오길 기대한다\n",
    "  - nn.linear가 채널 수를 input으로 받는 이유\n",
    "    - 데이터의 개수는 여러번 반복해서 들어가는 것이지, 노드의 수를 결정하는 것은 채널의 수이기 때문\n",
    "  - x가 1개 3채널이므로, nn.linear(3, ?) 넣어주듯이 채널이 들어오길 기대한다\n",
    "- nn.linear 같은 경우, weight가 개x채의 형태를 띄게끔 한다\n",
    "- 여기서 입력된 채널 수와 동일한 채널을 가진다\n",
    "  - nn.linear(3, 5) => 입력된 3 채널 (input)\n",
    "  - 이와 동일한 채널 수를 먼저 weight가 구성된다 > ?x3 (개x채)\n",
    "  - 그리고 나머지 5개의 노드 수를 맞추기 위해서 > 5x3 (개x채) 가 된다\n",
    "- 이를 위해서 Transpose가 들어가 있는 것 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6183], grad_fn=<AddBackward0>)\n",
      "\n",
      "----- shape -----\n",
      "x.shape : torch.Size([1])\n",
      "fc1.weight : torch.Size([3, 1])\n",
      "fc1.bias : torch.Size([3])\n",
      "fc2.weight : torch.Size([1, 3])\n",
      "fc2.bias : torch.Size([1])\n",
      "y.shape : torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "y = (x@fc1.weight.T + fc1.bias)@fc2.weight.T + fc2.bias\n",
    "## (1x1 @ 1x3 + 1x3)@ 3x1 + 1x1)\n",
    "print(y)\n",
    "print()\n",
    "\n",
    "print(\"----- shape -----\")\n",
    "print(f\"x.shape : {x.shape}\")\n",
    "print(f\"fc1.weight : {fc1.weight.shape}\")\n",
    "print(f\"fc1.bias : {fc1.bias.shape}\")\n",
    "print(f\"fc2.weight : {fc2.weight.shape}\")\n",
    "print(f\"fc2.bias : {fc2.bias.shape}\")\n",
    "print(f\"y.shape : {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0465], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.])\n",
    "\n",
    "fc1 = nn.Linear(1,3)\n",
    "fc2 = nn.Linear(3,1)\n",
    "\n",
    "model = nn.Sequential(fc1, fc2) # layer 붙이기!\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0241, -0.2140,  0.3608], grad_fn=<AddBackward0>)\n",
      "tensor([[ 0.1444, -0.0622,  0.1991],\n",
      "        [ 0.0230,  0.0630,  0.4521],\n",
      "        [-0.0132, -0.0383,  0.4829],\n",
      "        [ 0.2429, -0.0606,  0.0270],\n",
      "        [-0.0350,  0.2577,  0.6164]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([3, 4, 5, 2, 3, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(2, 5),\n",
    "                      nn.Linear(5, 10),\n",
    "                      nn.Linear(10, 3))\n",
    "\n",
    "x = torch.randn(2)\n",
    "print(model(x)) # 1x2 2x5 5x10 10x3 = 1x3\n",
    "\n",
    "x = torch.randn(5,2) # 5개 2채널 (개x채)\n",
    "print(model(x)) # 채널 수가 들어가 나오므로, 5x3\n",
    "\n",
    "x = torch.randn(3, 4, 5, 2, 3, 5,2) # 앞 개수가 계속 추가되어도 가능하다!\n",
    "print(model(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model class 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본틀!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mymodel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "  def forward(self, x):\n",
    "    pass\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mymodel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    self.fc1 = nn.Linear(3,5)\n",
    "    self.fc2 = nn.Linear(5,10)\n",
    "    self.fc3 = nn.Linear(10,2)\n",
    "    self.sg1 = nn.Sigmoid()\n",
    "    self.sg2 = nn.Sigmoid()\n",
    "    self.sg3 = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.fc1(x)\n",
    "    x = self.sg1(x)\n",
    "    x = self.fc2(x)\n",
    "    x = self.sg2(x)\n",
    "    x = self.fc3(x)\n",
    "    x = self.sg3(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5499, 0.4168],\n",
       "        [0.5472, 0.4179],\n",
       "        [0.5491, 0.4182],\n",
       "        [0.5497, 0.4193],\n",
       "        [0.5510, 0.4172]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Mymodel()\n",
    "x = torch.randn(5, 3)\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mymodel(\n",
      "  (fc1): Linear(in_features=3, out_features=5, bias=True)\n",
      "  (fc2): Linear(in_features=5, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=2, bias=True)\n",
      "  (sg1): Sigmoid()\n",
      "  (sg2): Sigmoid()\n",
      "  (sg3): Sigmoid()\n",
      ")\n",
      "Linear(in_features=3, out_features=5, bias=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.5597, -0.4522, -0.3611],\n",
      "        [-0.0340, -0.2003, -0.0444],\n",
      "        [-0.3687, -0.5011,  0.0886],\n",
      "        [ 0.4488,  0.0336,  0.3306],\n",
      "        [-0.0227,  0.3573,  0.4370]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 모델 확인\n",
    "print(model)\n",
    "print(model.fc1)\n",
    "print(model.fc1.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential 사용\n",
    "class Mymodel2(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    # self.fc1 = nn.Linear(3,5)\n",
    "    # self.fc2 = nn.Linear(5,10)\n",
    "    # self.fc3 = nn.Linear(10,2)\n",
    "    # self.sg1 = nn.Sigmoid()\n",
    "    # self.sg2 = nn.Sigmoid()\n",
    "    # self.sg3 = nn.Sigmoid()\n",
    "    self.linear = nn.Sequential(\n",
    "      nn.Linear(3,5),\n",
    "      nn.Sigmoid(),\n",
    "      nn.Linear(5,10),\n",
    "      nn.Sigmoid(),\n",
    "      nn.Linear(10,2),\n",
    "      nn.Sigmoid(),\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.linear(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4159, 0.5959],\n",
       "        [0.4160, 0.5965],\n",
       "        [0.4129, 0.5981],\n",
       "        [0.4150, 0.5967],\n",
       "        [0.4158, 0.5975]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Mymodel2()\n",
    "x = torch.randn(5, 3)\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mymodel2(\n",
      "  (linear): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=5, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=5, out_features=10, bias=True)\n",
      "    (3): Sigmoid()\n",
      "    (4): Linear(in_features=10, out_features=2, bias=True)\n",
      "    (5): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Parameter containing:\n",
      "tensor([[-0.1965, -0.2415, -0.2549],\n",
      "        [ 0.2851,  0.1767,  0.0094],\n",
      "        [ 0.5686,  0.0639, -0.1442],\n",
      "        [ 0.4942,  0.2143, -0.1192],\n",
      "        [ 0.3570,  0.0703, -0.3899]], requires_grad=True)\n",
      "Sigmoid()\n"
     ]
    }
   ],
   "source": [
    "# 모델 확인\n",
    "print(model)\n",
    "print(model.linear[0].weight) # 인덱스로 fc1 접근\n",
    "print(model.linear[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.1965, -0.2415, -0.2549],\n",
       "         [ 0.2851,  0.1767,  0.0094],\n",
       "         [ 0.5686,  0.0639, -0.1442],\n",
       "         [ 0.4942,  0.2143, -0.1192],\n",
       "         [ 0.3570,  0.0703, -0.3899]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.2856, 0.2087, 0.4699, 0.3086, 0.4645], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.3447, -0.1969, -0.3284, -0.3139,  0.0427],\n",
       "         [-0.1375, -0.1848, -0.2784, -0.4447,  0.1528],\n",
       "         [-0.0649,  0.0055, -0.4182,  0.2248, -0.2208],\n",
       "         [ 0.1901,  0.1547, -0.3315,  0.2063,  0.0673],\n",
       "         [-0.2925, -0.4321,  0.3483,  0.1526, -0.0471],\n",
       "         [-0.2469,  0.0486,  0.0498, -0.4134, -0.2500],\n",
       "         [ 0.2269, -0.0231,  0.3452,  0.3079,  0.3826],\n",
       "         [ 0.1112, -0.1615,  0.4073, -0.4265,  0.2303],\n",
       "         [-0.1285,  0.0678,  0.1108,  0.0757, -0.1835],\n",
       "         [-0.1418,  0.2732,  0.2416,  0.4188,  0.1994]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0014,  0.1581,  0.0352, -0.4125,  0.1311,  0.1730,  0.3196,  0.0773,\n",
       "         -0.3126, -0.3054], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.1546,  0.1500, -0.1205,  0.0956,  0.1459, -0.1754, -0.2038, -0.3003,\n",
       "          -0.1037,  0.3035],\n",
       "         [ 0.2774,  0.0304, -0.0542,  0.0238, -0.0628,  0.0282,  0.1261,  0.1832,\n",
       "           0.1881, -0.2989]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.2578,  0.2169], requires_grad=True)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 안에 있는 모든 파라미터 확인\n",
    "list(model.parameters()) # weight, bias 순서로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 5, 50, 10, 20, 2]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 파라미터의 원소 개수 확인\n",
    "[i.numel() for i in model.parameters()] # weight, bias 순서로"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight initialization을 직접할 수 있다 !\n",
    "# He initialization 이 공식 문서랑 다르다? => paper에 맞게 구현됐고 torch 공식 문서만 틀림 ???\n",
    "\n",
    "Fin=5000\n",
    "Fout=1000\n",
    "w = torch.zeros(141, Fin)\n",
    "nn.init.kaiming_uniform_(w, mode='fan_in', nonlinearity='relu') # forward pass에서 값의 범위를 유지시켜주기 위함\n",
    "print(w.std())\n",
    "print(torch.sqrt(torch.tensor(2/Fin)))\n",
    "w = torch.zeros(Fout, 212)\n",
    "nn.init.kaiming_uniform_(w, mode='fan_out', nonlinearity='relu') # backward pass에서 값의 범위를 유지시켜주기 위함\n",
    "print(w.std())\n",
    "print(torch.sqrt(torch.tensor(2/Fout)))\n",
    "\n",
    "# CNN?\n",
    "N=32\n",
    "C=64\n",
    "H=6\n",
    "W=10\n",
    "w = torch.zeros(N,C,H,W)\n",
    "nn.init.kaiming_uniform_(w, mode='fan_in', nonlinearity='relu')\n",
    "print(w.std())\n",
    "print(torch.sqrt(torch.tensor(2/(C*H*W))))\n",
    "\n",
    "w = torch.zeros(N,C,H,W)\n",
    "nn.init.kaiming_uniform_(w, mode='fan_out', nonlinearity='relu')\n",
    "print(w.std())\n",
    "print(torch.sqrt(torch.tensor(2/(N*H*W))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1:19:16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
