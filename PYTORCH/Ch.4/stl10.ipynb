{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from multiclass_functions2_8 import * # all\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "# import albumentations as A\n",
    "# from albumentations.pytorch import ToTensorV2\n",
    "# import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "LR = 2e-3\n",
    "LR_STEP = 3\n",
    "LR_GAMMA = 0.9\n",
    "EPOCH = 10\n",
    "TRAIN_RATIO = 0.8\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "new_model_train = False\n",
    "model_type = \"CNN_deep\"\n",
    "dataset = \"STL10\"\n",
    "save_model_path = f\"save_model/{model_type}_{dataset}.pt\"\n",
    "save_history_path = f\"save_model/{model_type}_history_{dataset}.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_uint8(x):\n",
    "    return (255*x).type(torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "\n",
    "    # transforms.Lambda(to_uint8),\n",
    "    # to_uint8, # 가능\n",
    "    # transforms.Lambda(lambda x:(255*x).type(torch.uint8)),\n",
    "    # lambda x:(255*x).type(torch.uint8), # 가능\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    # (아마도) 이미지넷 데이터 전체 픽셀에 대해서 구한 평균, std 값\n",
    "\n",
    "    # transforms.Resize(size=(20,20)), # size는 (height,width)\n",
    "    # transforms.CenterCrop(size=(20,20)), # size 는 (height, width)\n",
    "    # transforms.Pad(6), # 20+6*2 = 32\n",
    "    # transforms.RandomApply(nn.ModuleList([transforms.CenterCrop(size=(20,20)),\n",
    "    #                                       transforms.Pad(6)]), p=0.5), # ModuleList 안해도 되더라\n",
    "    # transforms.RandomCrop(size=(20,20)), # 어디를 자를지 random 하게 자름\n",
    "    # transforms.RandomResizedCrop(size=(20,20), scale=(0.3,1), ratio=(0.3,1.7)),\n",
    "    # random 하게 자른 다음에 resize라서 RandomCrop과 다름\n",
    "    # scale은 어느 정도로 작은 범위를 자를지 (0~1 사이 값)\n",
    "    # ratio는 aspect ratio로, 가로 길이/세로 길이 를 의미한다.\n",
    "    # (a,b) <- a 와 b 사이 값을 uniform random 하게 뽑아요\n",
    "\n",
    "    # transforms.RandomGrayscale(p=0.5), # gray로 바꾸는 데, 출력 채널 수를 3으로 유지\n",
    "    # transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.5, hue=0.15),\n",
    "    # 밝기, 대비, 채도, 색조가 센서마다 다를 수 있음을 고려 (number: percentage to convert)\n",
    "    # 밝기(brightness)를 키우면 밝은 부분에서 saturation이 일어나면서, HDR이 낮은 센서에 대응 가능\n",
    "    # 대비(contrast) augmentation은 HDR이 서로 다른 센서에 대응 가능\n",
    "    # 채도(saturation) 및 색조(hue) augmentation은 ISP에서 color correction matrix 값이 센서마다 서로 다른 경우 등,\n",
    "    # 색 표현이 다른 센서에 대응 가능\n",
    "    # transforms.RandomAutocontrast(p=0.5), # p의 확률로 대비를 자동 조정\n",
    "    # transforms.RandomEqualize(p=0.5), # p의 확률로 R,G,B histogram을 일치시킴 (이건 uint8로 되어있는 이미지만 가능)\n",
    "    # transforms.RandomInvert(p=0.5), # p의 확률로 색반전\n",
    "    # new 픽셀 값 = 최댓 값 - 기존의 픽셀 값 을 통해 반전시킴\n",
    "    # transforms.RandomSolarize(threshold=200, p=0.5),\n",
    "    # p의 확률로(즉 그 이미지에 대해 할지 말지 정하고) threshold 넘는 픽셀 값에 대해 inverting\n",
    "    # transforms.RandomPosterize(bits=3, p=0.5),\n",
    "    # 픽셀 하나가 가지는 값의 bit수(0~8)를 p의 확률로 bits로 바꿈 (이건 uint8로 되어있는 것만 가능)\n",
    "\n",
    "    # transforms.GaussianBlur(kernel_size=(5,5), sigma=(0.1,2)),\n",
    "    # kernel_size = (가로,세로) sigma=(min,max) min ~ max 에서 uniform 하게 하나 뽑음\n",
    "    # transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.5),\n",
    "    # sharpness_factor = 1 이면 원래 이미지, 0에 가까우면 blur, 1보다 클수록 sharp해짐\n",
    "\n",
    "    # transforms.RandomHorizontalFlip(p=0.4), # p 확률로 좌우반전\n",
    "    # transforms.RandomVerticalFlip(p=0.5), # p 확률로 상하반전\n",
    "    # transforms.RandomRotation(degrees=(0,180)), # 0~180도 랜덤하게 회전\n",
    "    # transforms.RandomAffine(degrees=(0,30),translate=(0.1,0.3),scale=(0.5,1.2)),\n",
    "    # translate는 이동하는 정도 (비율), scale은 크기 조절\n",
    "    # transforms.RandomPerspective(distortion_scale=0.6, p=0.5), # distortion 정도 0~1 사이, default는 0.5, p는 적용 확률\n",
    "\n",
    "    # transforms.RandomErasing(p=0.5, scale=(0.03,0.3), ratio=(0.3,3.3)),\n",
    "    # scale: 이미지의 몇 퍼 정도를 지울지\n",
    "    # ratio: 지우는 영역의 aspect ratio\n",
    "\n",
    "    # transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10),\n",
    "    # transforms.AutoAugment(transforms.AutoAugmentPolicy.IMAGENET),\n",
    "    # transforms.AutoAugment(transforms.AutoAugmentPolicy.SVHN),\n",
    "    # AutoAugment:Learning Augmentation Strategies from Data 라는 논문에 따른 augmentation (uint8 이여야 함)\n",
    "    # transforms.RandAugment(),\n",
    "    # RandAugment: Practical automated data augmentation with a reduced search space 라는 논문에 따른 augmentation (uint8 이여야 함)\n",
    "    # transforms.TrivialAugmentWide(),\n",
    "    # TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation 라는 논문에 따른 augmentation (uint8)\n",
    "    # transforms.AugMix(),\n",
    "    # AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty 라는 논문에 따른 augmentaiton (uint8)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train, val dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_DS = datasets.CIFAR10(root = '../data', train=True, download=True, transform=transform_train)\n",
    "test_DS = datasets.CIFAR10(root = '../data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# train, val num\n",
    "NoT = int(len(train_DS)*TRAIN_RATIO)\n",
    "NoV = len(train_DS) - NoT\n",
    "train_DS, val_DS = torch.utils.data.random_split(train_DS, [NoT, NoV])\n",
    "\n",
    "val_DS.transform = transform_test # val에는 test transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_DL = torch.utils.data.DataLoader(train_DS, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_DL = torch.utils.data.DataLoader(val_DS, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_DL = torch.utils.data.DataLoader(test_DS, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Albumentation 쓸 때만 주석 해제\n",
    "\n",
    "transform_train = A.Compose([\n",
    "    A.Resize(20,20),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "\n",
    "    A.ShiftScaleRotate(shift_limit=(-0.05,0.05), scale_limit=(0,0), rotate_limit=(-10,10), p=0.7, border_mode=cv2.BORDER_CONSTANT),\n",
    "    A.RandomResizedCrop(height=20, width=20, scale=(0.3, 1), ratio=(0.75, 1.33), p=0.7),\n",
    "\n",
    "    A.RandomBrightnessContrast(brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), p=0.8),\n",
    "\n",
    "    A.OneOf([\n",
    "             A.ElasticTransform(p=1, sigma=30, alpha=10, alpha_affine=5),\n",
    "             #  sigma는 간격, alpha는 꼬불한 정도, alpla_affine은 affine transform의 정도\n",
    "             A.GridDistortion(p=1), # grid로 쪼갠다음 각 patch들을 짜부시킴\n",
    "             A.OpticalDistortion(p=1,distort_limit=1, shift_limit=0.5),\n",
    "             #  distort_limit 가운데 부분이 볼록 튀어나와 보이는 정도 (0~1), shift_limit은 어느정도 이미지를 밀건지 (0~1)\n",
    "             ], p=1),\n",
    "\n",
    "    # A.Normalize(), # 애는 default가 0~255인 이미지가 들어오는 것으로 되어있어서 normalize 먼저\n",
    "    ToTensorV2()])\n",
    "\n",
    "transform_test = ToTensorV2() # unit8, 0~255는 그대로, Tensor, 개채행열로만 바꿔줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10_custom(datasets.CIFAR10):\n",
    "    def __init__(self, root=\"/content/drive/MyDrive/Colab Notebooks/data\", train=True, download=True, transform=None):\n",
    "        super().__init__(root=root, train=train, download=download, transform=transform)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.data[index]\n",
    "        label = self.targets[index]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            transformed = self.transform(image=image) # self.transform(image=image, mask=mask)\n",
    "            image = transformed[\"image\"]\n",
    "            # mask = transformed[\"mask\"]\n",
    "\n",
    "        return image, label\n",
    "\n",
    "test_DS = CIFAR10_custom(root = '/content/drive/MyDrive/Colab Notebooks/data', train=False, download=True, transform=transform_test)\n",
    "test_DL = torch.utils.data.DataLoader(test_DS, batch_size = BATCH_SIZE, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_DS_wT = copy.deepcopy(test_DS) # wT = with Transform\n",
    "test_DS_wT.transform = transform_train\n",
    "test_DL_wT = torch.utils.data.DataLoader(test_DS_wT, batch_size = BATCH_SIZE, shuffle = False)\n",
    "\n",
    "im_plot(test_DL)\n",
    "im_plot(test_DL_wT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_DS)\n",
    "print(test_DS)\n",
    "print(len(train_DS))\n",
    "print(len(test_DS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_DS.classes)\n",
    "x_batch, y_batch = next(iter(test_DL))\n",
    "print(x_batch.shape)\n",
    "plt.imshow(x_batch[0].permute(1,2,0))\n",
    "print(test_DS.classes[y_batch[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_deep(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_block1 = nn.Sequential(nn.Conv2d(3,32,3,padding=1),\n",
    "                                         nn.BatchNorm2d(32),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Conv2d(32,32,3,padding=1),\n",
    "                                         nn.BatchNorm2d(32),\n",
    "                                         nn.ReLU())\n",
    "        self.Maxpool1 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv_block2 = nn.Sequential(nn.Conv2d(32,64,3,padding=1),\n",
    "                                         nn.BatchNorm2d(64),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Conv2d(64,64,3,padding=1),\n",
    "                                         nn.BatchNorm2d(64),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Conv2d(64,64,3,padding=1),\n",
    "                                         nn.BatchNorm2d(64),\n",
    "                                         nn.ReLU())\n",
    "        self.Maxpool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv_block3 = nn.Sequential(nn.Conv2d(64,128,3,padding=1),\n",
    "                                         nn.BatchNorm2d(128),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Conv2d(128,128,3,padding=1),\n",
    "                                         nn.BatchNorm2d(128),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Conv2d(128,128,3,padding=1),\n",
    "                                         nn.BatchNorm2d(128),\n",
    "                                         nn.ReLU())\n",
    "        self.Maxpool3 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.classifier = nn.Sequential(nn.Linear(128*12*12,512),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(512,10))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.Maxpool1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.Maxpool2(x)\n",
    "        x = self.conv_block3(x)\n",
    "        x = self.Maxpool3(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(f\"model = {model_type}().to(DEVICE)\")\n",
    "print(model)\n",
    "x_batch, _ = next(iter(train_DL))\n",
    "print(model(x_batch.to(DEVICE)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if new_model_train:\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    loss_history = Train(model, train_DL, criterion, optimizer, EPOCH)\n",
    "\n",
    "    torch.save(model, save_model_path)\n",
    "\n",
    "    plt.plot(range(1,EPOCH+1),loss_history)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title(\"Train Loss\")\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = torch.load(save_model_path, map_location=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test(load_model, test_DL)\n",
    "print(count_params(load_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_plot(load_model, test_DL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "epoch_start = time.time()\n",
    "for _ in range(100000):\n",
    "    a=1\n",
    "time.time()-epoch_start"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
