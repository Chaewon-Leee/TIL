{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import load_boston\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lee/opt/anaconda3/envs/DL/lib/python3.8/site-packages/sklearn/datasets/_openml.py:292: UserWarning: Multiple active versions of the dataset matching the name boston exist. Versions may be fundamentally different, returning version 1.\n",
      "  warn(\n",
      "/Users/lee/opt/anaconda3/envs/DL/lib/python3.8/site-packages/sklearn/datasets/_openml.py:932: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# boston = load_boston()\n",
    "\n",
    "# x_data = boston.data\n",
    "# y_data = boston.target.reshape(boston.target.size, 1)\n",
    "\n",
    "x_data, y_data = datasets.fetch_openml('boston', return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 9.00000000e-01, 3.39076246e-01, 0.00000000e+00,\n",
       "        1.57407407e+00, 2.88752635e+00, 3.20803296e+00, 1.34601570e+00,\n",
       "        0.00000000e+00, 1.04007634e+00, 1.43617021e+00, 5.00000000e+00,\n",
       "        4.48399558e-01],\n",
       "       [1.17961270e-03, 0.00000000e+00, 1.21151026e+00, 0.00000000e+00,\n",
       "        8.64197531e-01, 2.73998850e+00, 3.91349125e+00, 1.74480990e+00,\n",
       "        2.17391304e-01, 5.24809160e-01, 2.76595745e+00, 5.00000000e+00,\n",
       "        1.02235099e+00],\n",
       "       [1.17848872e-03, 0.00000000e+00, 1.21151026e+00, 0.00000000e+00,\n",
       "        8.64197531e-01, 3.47192949e+00, 2.99691040e+00, 1.74480990e+00,\n",
       "        2.17391304e-01, 5.24809160e-01, 2.76595745e+00, 4.94868627e+00,\n",
       "        3.17328918e-01]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "minmax_scale = preprocessing.MinMaxScaler(feature_range=(0,5)).fit(x_data) # minmax를 변환시켜줄 수 있는 함수를 만드는 것\n",
    "  # feature_range=(0,5) : 0~5사이 값으로 scaling 이루어짐\n",
    "# standard_scale = preprocessing.StandardScaler().fit(x_data) # standardzation\n",
    "x_scaled_data = minmax_scale.transform(x_data)\n",
    "\n",
    "x_scaled_data[:3] # 스케일링이 된 x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_scaled_data, y_data, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((339, 13), (167, 13), (339,), (167,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients:  [-1.97744661  1.03249931  0.31185481  0.64202747 -1.47298249  4.26477538\n",
      " -0.05511343 -2.94372093  1.45991381 -1.26022797 -1.93424561  0.68856781\n",
      " -3.79570474]\n",
      "intercept:  25.745857480575857\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "regr = linear_model.LinearRegression(fit_intercept=True, \n",
    "                                    copy_X=True, \n",
    "                                    # normalize=False, \n",
    "                                    n_jobs=8)\n",
    "# fit_intercept : 절편값을 넣을 것인지, copy_X : 모델을 학습하다보면 x 값이 변환될 수 있어서 x를 복사해서 학습시킬건지 여부, n_jobs : 몇개의 지표를 사용해서 할 것인지\n",
    "\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "# # The coefficients\n",
    "print('Coefficients: ', regr.coef_) # W1 ~ W13 값\n",
    "print('intercept: ', regr.intercept_)  # W0 = 절편 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13.40070722, 13.93904081, 27.4501785 , 13.95028563, 16.01208379])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.predict(X_test[:5]) \n",
    "# 알고 싶은 데이터를 넣게 될 경우, 그에 따른 predict 값을 출력해준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13.40070722, 13.93904081, 27.4501785 , 13.95028563, 16.01208379])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:5].dot(regr.coef_.T) + regr.intercept_ # 실제 coef_와 X의 곱과 절편을 더해준 것과 동일한 값이 나온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7515206497067568, 3.070834599791317, 15.754390532729532)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = y_test\n",
    "y_hat = regr.predict(X_test)\n",
    "\n",
    "r2_score(y_true, y_hat), mean_absolute_error(y_true, y_hat), mean_squared_error(y_true, y_hat) # 실제 값과 예측한 값 성능 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
